{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cce13ac",
   "metadata": {},
   "source": [
    "Progetto: **Classificazione di testi descrittivi per destinazione d'uso, inerenti ai beni sottoposti ad aste giudiziarie italiane.**\n",
    "\n",
    "Studente: **Alessandro Monolo** | *10439147*\n",
    "\n",
    "Relatore: Marco Brambilla\n",
    "\n",
    "Referente aziendale: Simone Redaelli\n",
    "\n",
    "Master: Data Science & Artificial Intelligence\n",
    "\n",
    "Università: Politecnico di Milano\n",
    "\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "## Seconda parte di Feature Engineering e Pre - Processing sulla variabile testuale\n",
    "\n",
    "- **1.** **<u>Feature enginering</u>**:\n",
    "    - **1.1** **Creazione di una nuova colonna** che conta il numero di **StopWords** per descrizione;\n",
    "    - **1.2** **Creazione di una nuova colonna** che conta il numero di **Cifre** presenti per descrizione;\n",
    "    - **1.3** **Creazione di una nuova colonna** che conta il numero di **Upper case words** per descrizione;\n",
    "\n",
    "\n",
    "- **2.** **<u>Pre-Processing sulla variabile testuale:</u>**\n",
    "    - Per la fase di pre-processing sulla variabile testuale invece, ho eseguito finora la pulizia del testo, togliendo:\n",
    "        - **2.1** **e-mails**;\n",
    "        - **2.2** **URLs**;\n",
    "        - **2.3** **Doppi spazi o spazi multipli**;\n",
    "        - **2.4** **Lower casa only**;\n",
    "        - **2.5** **Punctuation**;\n",
    "        - **2.6** **Stopwords**;\n",
    "        - **2.7** **Caratteri accentati**;\n",
    "        - **2.8** **Di nuovo - Doppi spazi o spazi multipli**;\n",
    "        - **2.9** **Rimozione delle parole troppo corte, sotto i 2 caratteri**;\n",
    "        - **2.10** **Rimozione parole non comuni**;\n",
    "        \n",
    "\n",
    "\n",
    "- **3. <u>Pre-processo la variabile testuale attraverso la libreria denominata Spacy per la fase di Lemmatization</u>**\n",
    "\n",
    "\n",
    "- **4.** **<u>Fase di Pre-Processign per la Target Variable \"Destinazione d'uso\", applicando LabelEncoder</u>**;\n",
    "\n",
    "\n",
    "- **5.** **<u>Filtro il data frame</u>**;\n",
    "\n",
    "\n",
    "- **6.** **<u>Conclusioni</u>**\n",
    "\n",
    "\n",
    "- **7.** **<u>Export final CSV</u>**\n",
    "\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "**Importo le librerie che mi servono:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74efb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import math\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d056111",
   "metadata": {},
   "source": [
    "#### Set pandas options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aaadd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_rows = 5000\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70e184",
   "metadata": {},
   "source": [
    "**Importo file CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe04dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\Master_Cefriel_DS_AI_Monolo\\\\0_Project_Work\\\\Dataset\\\\10_Dataset_Pre_Processing_Var_Text\\\\Dataset_Pre_Processing_Var_Text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182ee70",
   "metadata": {},
   "source": [
    "### 1 - Feature Enginering\n",
    "\n",
    "- In questo caso, come per le altre colonne create dalla variabile testuale, utilizzo il campo \"Descrizione_Bene_Clean\", ovvero il campo descrittivo senza i simboli strani, ma con ancora sia la punctuation sia le capital letters.\n",
    "\n",
    "#### 1.1 Conto il numero di stop words per descrizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aabcebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanzio il vocabolario delle stopwords - Italian:\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "# Creo la nuova colonna:\n",
    "df['N_Stop_Words'] = df[\"Descrizione_Bene_Clean\"].apply(lambda x : len([t for t in x.split() if t in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705954a",
   "metadata": {},
   "source": [
    "#### 1.2 Conto il numero di cifre presenti per descrizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab69109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la nuova colonna:\n",
    "df[\"N_Numeri\"] = df[\"Descrizione_Bene_Clean\"].apply(lambda x: len([t for t in x.split() if t.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e514c",
   "metadata": {},
   "source": [
    "#### 1.3 Conto il numero di stringhe in upper case per descrizione\n",
    "\n",
    "- Per questa nuova colonna sono abbastanza dubbioso in quanto non esprimerebbe nulla. La presenza in alcuni testi delle upper case words è totalmente casuale in quanto i testi sono tutti in upper case alle volte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "477c59fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la nuova colonna:\n",
    "df[\"N_Upper_Word\"] = df[\"Descrizione_Bene_Clean\"].apply(lambda x: len([t for t in x.split() if t.isupper()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb82eb",
   "metadata": {},
   "source": [
    "### 2 - Text Pre-Processing\n",
    "\n",
    "- **2.1 Trovo tutte le emails nel campo descrizione, le conto ed infine le elimino dal testo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15bacea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    165878\n",
       "1        91\n",
       "3        20\n",
       "2        11\n",
       "4         3\n",
       "Name: N_Emails, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creo una nuova colonna che estragga tutte le email dal testo:\n",
    "df[\"Emails\"] = df[\"Descrizione_Bene_Clean\"].apply(lambda x: re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+._-]+)', x))\n",
    "\n",
    "# Creo un anuova colonna che conti il numero di mail dal testo:\n",
    "df[\"N_Emails\"] = df[\"Emails\"].apply(lambda x: len(x))\n",
    "\n",
    "# Elimino dal testo tutte le mail e salvo in una nuova colonna il testo pulito:\n",
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Clean\"].apply(lambda x: re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+._-]+)', '', x))\n",
    "\n",
    "# Quantifico il numero di mails presenti nel campo descrizione del bene:\n",
    "df[\"N_Emails\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b32137",
   "metadata": {},
   "source": [
    "- Solo in **125 casi su 166.003**, il testo presenta almeno **una mail**.\n",
    "- Solo in **23 casi** il testo della descrizione del bene presenta più di **2 mails**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828e5b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESIDENTIAL   0.7200\n",
      "INDUSTRIAL    0.0960\n",
      "STORAGE       0.0800\n",
      "RETAIL        0.0560\n",
      "LAND          0.0400\n",
      "HOTEL         0.0080\n",
      "Name: Destinazione_Uso, dtype: float64 \n",
      "\n",
      "RESIDENTIAL    90\n",
      "INDUSTRIAL     12\n",
      "STORAGE        10\n",
      "RETAIL          7\n",
      "LAND            5\n",
      "HOTEL           1\n",
      "Name: Destinazione_Uso, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_with_mails = df[df[\"N_Emails\"] > 0]\n",
    "print(df_with_mails[\"Destinazione_Uso\"].value_counts(1), \"\\n\")\n",
    "print(df_with_mails[\"Destinazione_Uso\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b108eb8",
   "metadata": {},
   "source": [
    "- Nel **72% dei casi le mails sono presenti nei beni con destinazione d'uso Residential**, mentre nel **10% dei casi le mails sono presenti nei beni Industrial**.\n",
    "- Il restante **18% dei casi, le mails sono presenti nei restanti tipi** di beni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009b74c",
   "metadata": {},
   "source": [
    "- **2.2 Trovo tutti i links/URL/Siti Web nel campo descrizione, conto ed infine le elimino dal testo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a20808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    166003\n",
       "Name: N_URL, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creo una nuova colonna che estragga tutti i links/URL/Siti Web dal testo:\n",
    "df[\"URL\"] = df[\"Descrizione_Bene_Clean\"].apply(lambda x: re.findall(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', x))\n",
    "\n",
    "# Creo un anuova colonna che conti il numero di links/URL/Siti Web dal testo:\n",
    "df[\"N_URL\"] = df[\"URL\"].apply(lambda x: len(x))\n",
    "\n",
    "# Elimino dal testo tutti i links/URL/Siti Web e salvo in una nuova colonna il testo pulito:\n",
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w•,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x))\n",
    "\n",
    "# Quantifico il numero di links estratti dai testi:\n",
    "df['N_URL'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d2dc2",
   "metadata": {},
   "source": [
    "- Si evince quindi che **non sono presenti link a URL nel campo descrizione** del bene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d5004",
   "metadata": {},
   "source": [
    "- **2.3 Elimino tutti gli spazi doppi o spazi multipli nel campo Descrizione del bene:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c0af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613b18c",
   "metadata": {},
   "source": [
    "- **2.4 Converto tutte le parole presenti nel campo descrizione del bene in lower case only:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d00ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f957990",
   "metadata": {},
   "source": [
    "- **2.5 Elimino la punctuation nel campo Descrizione del bene:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "882ea69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino dal testo tutti gli special characters:\n",
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: re.sub(r'[^\\w ]+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e48434",
   "metadata": {},
   "source": [
    "- **2.6 Elimino tutte le stopwords presenti nel campo descrizione del bene:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eea3ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setto il vocabolario in italiano per le stopwords in italiano:\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "\n",
    "# Applico la funzione stopwords alla colonna descrizione bene pulita e pre processata:\n",
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: ' '.join([t for t in x.split() if t not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f460ac",
   "metadata": {},
   "source": [
    "- **2.7 Elimino tutti i caratteri che presentano accento:**\n",
    "\n",
    "\n",
    "- Importante da considerare come questa fase di pulizia, sia necessaria per eliminare tutte quelle lettere che potrebbero deformare una parola o renderla malinterpretabile durante la fase di predizione. Inoltre, la libreria NLTK.StopWords non tiene conto durante la sua applicazione di testi accentati o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788d4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una funzione che mi permetta di eliminare tutte le accento presenti nei testi\n",
    "# di descrizione e le sostituisca con medesime lettere non accentuate:\n",
    "def remove_accented_chars(x):\n",
    "    x = unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    return x\n",
    "# Applico la funzione creata al campo descrizione pre processato.\n",
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: remove_accented_chars(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2c78d",
   "metadata": {},
   "source": [
    "- **2.8 Elimino di nuovo tutti gli spazi doppi o spazi multipli nel campo Descrizione del bene:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b92daa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e8def",
   "metadata": {},
   "source": [
    "- **2.9** **Elimino le parole troppo corte, sotto i due caratteri:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46fdc73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisco una user defined function che sia in grado di eliminare tutte le parole composte da un singolo carattere:\n",
    "def filter_single_char_words(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if len(word) > 1]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Applico la user defined function alla colonna testuale pre-processata:\n",
    "df['Descrizione_Bene_Pre_Process'] = df['Descrizione_Bene_Pre_Process'].apply(filter_single_char_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068f40d",
   "metadata": {},
   "source": [
    "- **2.10** **Elimino le parole meno frequenti che rappresentano errori di battitura o errori di merge tra parole:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb91ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metto in una variabile tutte le stringhe presenti nella colonna:\n",
    "text = \" \".join(df[\"Descrizione_Bene_Pre_Process\"])\n",
    "text = text.split()\n",
    "# Conto le stringhe contenute e le metto in una Pandas Series:\n",
    "freq_common = pd.Series(text).value_counts()\n",
    "# Prendo le 200 parole meno frequenti cge rappresentano tutte parole o numeri di nessun significato:\n",
    "rare_words = freq_common.tail(300)\n",
    "\n",
    "# Applico il drop alla colonna testuale pre-processata:\n",
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: \" \".join([t for t in x.split() if t not in rare_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a11d3",
   "metadata": {},
   "source": [
    "### 3 - Lemmatization text variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ad007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 166003/166003 [33:11<00:00, 83.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Importo il vocabolario italiano:\n",
    "nlp = spacy.load('it_core_news_sm')\n",
    "\n",
    "# Creo una UDF per applicare la libreria spacy alla variabile testuale preprocessata, utilizzando Stanza:\n",
    "def lemmatize_text_spacy(text_1):\n",
    "    doc_1 = nlp(text_1)\n",
    "    lemmatized_tokens_1 = [token.lemma_ for token in doc_1]\n",
    "    return \" \".join(lemmatized_tokens_1)\n",
    "\n",
    "tqdm.pandas()\n",
    "# Applico la UDF alla variabile testuale:\n",
    "df['Descrizione_Bene_Lemm_Spacy'] = df['Descrizione_Bene_Pre_Process'].progress_apply(lemmatize_text_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fd0fb",
   "metadata": {},
   "source": [
    "### 4 - Fase di Pre-Processign per la Target Variable \"Destinazione d'uso\", applicando LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9170f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanzio in una nuova variabile il modello per l'encoding della target variable:\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Applico il modello di encoding alla mia target variable e salvo il risultato in una nuova colonna:\n",
    "df['Destinazione_Uso_Encoded'] = label_encoder.fit_transform(df['Destinazione_Uso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23faa5c3",
   "metadata": {},
   "source": [
    "### 5 - Filtro il data frame con solo le colonne che mi servono per la text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d04f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleziono le colonne da eliminare\n",
    "columns_to_drop = ['Destinazione_Uso', \n",
    "                   'Categoria_Catastale', \n",
    "                   'Tribunale', \n",
    "                   'Provincia', \n",
    "                   'Comune', \n",
    "                   'Numero_Lotto', \n",
    "                   'Numero_Vani', \n",
    "                   'Superficie', \n",
    "                   'Rapporto_Vani_Superficie', \n",
    "                   'N_Caratteri',\n",
    "                   'N_Parole', \n",
    "                   'avg_word_len', 'N_Stop_Words', \n",
    "                   'N_Numeri', 'N_Upper_Word',\n",
    "                   'N_Emails', \n",
    "                   'N_URL',\n",
    "                   'Catasto_Fabbricati', \n",
    "                   'Descrizione_Bene',\n",
    "                   'Descrizione_Bene_Clean', \n",
    "                   'Descrizione_Bene_EDA', \n",
    "                   'Emails',\n",
    "                   'N_URL',\n",
    "                   'URL']\n",
    "# Mantengo solo le colonne che mi servono per la text classification:\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc071055",
   "metadata": {},
   "source": [
    "### 6 - Conlusioni\n",
    "\n",
    "- Dalla variabile testuale \"Descrizione_Beni_Clean\", ovvero la variabile testuale inerente alle descrizioni dei lotti puliti da caratteri speciali quali \"\\n\" e \"\\\\\", sono state create ulteriori variabili estraendo da ogni descrizione di beni:\n",
    "    - Il numero di stop words;\n",
    "    - Il numero di cifre menzionate;\n",
    "    - Il numero di upper case words;\n",
    "\n",
    "\n",
    "- Dalla medesima variabile testuale è stato poi possibile utilizzando delle regex, tutte le mail utilizzate e tutti i Link/Web Site/URLs presenti:\n",
    "    - Nello specifico, le mail presenti hanno le seguenti caratteristiche:\n",
    "        - Solo in 125 casi su 166.003, il testo presenta almeno una mail.\n",
    "        - Solo in 23 casi il testo della descrizione del bene presenta più di 2 mails.\n",
    "        - Nel 72% dei casi le mail sono presenti nei beni con destinazione d'uso Residential, mentre nel 10% dei casi le mails sono per i beni Industrial.\n",
    "        - Il restante 18% dei casi, le mails sono presenti nei restanti tipi di beni.\n",
    "    - Mentre per quanto riguarda i siti web, non esistono siti web all'interno di nessuna descrizione di bene presente nel data frame.\n",
    "\n",
    "\n",
    "- Una volta eliminate le estrazioni menzionate sopra, dalla variabile \"Descrizione_Bene_PRe_Process\", sono state eseguite le seguenti manovre di pre-processing del dato testuale:\n",
    "    - Eliminazione di tutti gli spazi doppi o spazi multipli;\n",
    "    - Convesrione tutte le parole presenti nel campo descrizione del bene in lower case;\n",
    "    - Eliminazione della punctuation;\n",
    "    - Eliminazione delle stop words, avendo settato il dizionario in modalità Italiana;\n",
    "    - Eliminazione di tutte le accento sui caratteri delle parole, in quanto i modelli di text classification non subiscono variazione se le stringhe presentano o meno caratteri con accenti;\n",
    "    - Eliminazione di tutte le parole che presentano meno di due caratteri;\n",
    "    - Infine, eliminazione delle 300 parole meno frequenti in quelle menzionate all'interno delle descrizioni dei beni;\n",
    "    \n",
    "\n",
    "- Successivamente, è stata creata una variabile testuale lemmatizzata, attraverso le libreria Spacy.\n",
    "  Entrambe le variabili, quella con lemmatization e quella con solo la fase di pre-process, sono esportate nel df finale per la classificazion dei testi.\n",
    "  Su tutte e due sarà fatta la classificazione e dopodichè si vedrà quale avrà ottenuto le metriche più alte.\n",
    "    \n",
    "    \n",
    "- Come passaggio finale è stata encoddata la variabile target Destinazione d'uso attraverso la libreria LAbelEncoder per rendere numerica la variabile ed infine filtrato il data frame salvando solo le variabili che mi servono per la text classification;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b15d6",
   "metadata": {},
   "source": [
    "### 7 - Export final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "756f4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = df.to_csv(\"D:\\\\Master_Cefriel_DS_AI_Monolo\\\\0_Project_Work\\\\Dataset\\\\11_Dataset_TFIDF_LR_&_SVM\\\\Dataset_TFIDF_LR_&_SVM.csv\", \n",
    "                   index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
