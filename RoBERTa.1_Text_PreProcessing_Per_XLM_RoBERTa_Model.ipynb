{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cce13ac",
   "metadata": {},
   "source": [
    "Progetto: **Classificazione di testi descrittivi per destinazione d'uso, inerenti ai beni sottoposti ad aste giudiziarie italiane.**\n",
    "\n",
    "Studente: **Alessandro Monolo** | *10439147*\n",
    "\n",
    "Relatore: Marco Brambilla\n",
    "\n",
    "Referente aziendale: Simone Redaelli\n",
    "\n",
    "Master: Data Science & Artificial Intelligence\n",
    "\n",
    "Università: Politecnico di Milano\n",
    "\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "## Pre-Processing sulla variabile testuale per il modello XLM RoBERTa\n",
    "\n",
    "- **1.** **<u>Pre-Processing sulla variabile testuale:</u>**\n",
    "    - Per la fase di pre-processing sulla variabile testuale invece, partidirettamente da una base testuale pulita dai caratteri strani ed poi ho eseguito i seguenti steps per la pulizia del testo, togliendo:\n",
    "        - **1.1** **e-mails**;\n",
    "        - **1.2** **URLs**;\n",
    "        - **1.3** **Doppi spazi o spazi multipli**;\n",
    "        - **1.4** **Lower casa only**;\n",
    "        - **1.5** **Caratteri accentati**;\n",
    "        - **1.6** **Di nuovo - Doppi spazi o spazi multipli**;\n",
    "        - **1.7** **Rimozione delle parole troppo corte, sotto i 2 caratteri**;\n",
    "        - **1.8** **Elimino tutte le parole che presentano numeri in esse**, **ERRATO**\n",
    "        - **1.9** **Rimozione parole non comuni**;\n",
    "        \n",
    "\n",
    "- **2.** **<u>Fase di Pre-Processign per la Target Variable \"Destinazione d'uso\", applicando LabelEncoder</u>**;\n",
    "\n",
    "\n",
    "- **3.** **<u>Filtro il data frame</u>**;\n",
    "\n",
    "\n",
    "- **5.** **<u>Export final CSV</u>**\n",
    "\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "**Importo le librerie che mi servono:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74efb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import math\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d056111",
   "metadata": {},
   "source": [
    "#### Set pandas options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aaadd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_rows = 5000\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70e184",
   "metadata": {},
   "source": [
    "**Importo file CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe04dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\Master_Cefriel_DS_AI_Monolo\\\\0_Project_Work\\\\Dataset\\\\10_Dataset_Pre_Processing_Var_Text\\\\Dataset_Pre_Processing_Var_Text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c47cad",
   "metadata": {},
   "source": [
    "### 1 - Text Pre-Processing\n",
    "\n",
    "- **1.1 Trovo tutte le emails nel campo descrizione e le elimino dal testo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ace27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Clean\"].apply(lambda x: re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+._-]+)', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b6b65",
   "metadata": {},
   "source": [
    "- **1.2 Trovo tutti i links/URL/Siti Web nel campo descrizione e li elimino dal testo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0974e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w•,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d5004",
   "metadata": {},
   "source": [
    "- **1.3 Elimino tutti gli spazi doppi o spazi multipli nel campo Descrizione del bene:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c0af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613b18c",
   "metadata": {},
   "source": [
    "- **1.4 Converto tutte le parole presenti nel campo descrizione del bene in lower case only:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d00ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d503f",
   "metadata": {},
   "source": [
    "- **1.5 Elimino tutti i caratteri che presentano accento:**\n",
    "\n",
    "\n",
    "- Importante da considerare come questa fase di pulizia, sia necessaria per eliminare tutte quelle lettere che potrebbero deformare una parola o renderla malinterpretabile durante la fase di predizione."
   ]
  },
  {
   "cell_type": "raw",
   "id": "22425c06",
   "metadata": {},
   "source": [
    "def remove_accented_chars(x):\n",
    "    x = unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    return x\n",
    "\n",
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: remove_accented_chars(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06174c4a",
   "metadata": {},
   "source": [
    "- **1.8 Elimino tutte le parole che presentano numeri in esse:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d97a1aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_with_cypher(text):\n",
    "    return ' '.join(word for word in text.split() if not re.search(r'\\w*\\d\\w*', word))\n",
    "\n",
    "# Apply the function to the column\n",
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(remove_words_with_cypher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2c78d",
   "metadata": {},
   "source": [
    "- **1.6 Elimino di nuovo tutti gli spazi doppi o spazi multipli nel campo Descrizione del bene:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41283e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e8def",
   "metadata": {},
   "source": [
    "- **1.7** **Elimino le parole troppo corte, sotto i due caratteri:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86c80e97",
   "metadata": {},
   "source": [
    "def filter_single_char_words(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if len(word) > 1]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['Descrizione_Bene_Pre_Process'] = df['Descrizione_Bene_Pre_Process'].apply(filter_single_char_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068f40d",
   "metadata": {},
   "source": [
    "- **1.9** **Elimino le parole meno frequenti che rappresentano errori di battitura o errori di merge tra parole:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb91ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metto in una variabile tutte le stringhe presenti nella colonna:\n",
    "text = \" \".join(df[\"Descrizione_Bene_Pre_Process\"])\n",
    "text = text.split()\n",
    "# Conto le stringhe contenute e le metto in una Pandas Series:\n",
    "freq_common = pd.Series(text).value_counts()\n",
    "# Prendo le 200 parole meno frequenti cge rappresentano tutte parole o numeri di nessun significato:\n",
    "rare_words = freq_common.tail(500)\n",
    "\n",
    "# Applico il drop alla colonna testuale pre-processata:\n",
    "df[\"Descrizione_Bene_Pre_Process\"] = df[\"Descrizione_Bene_Pre_Process\"].apply(lambda x: \" \".join([t for t in x.split() if t not in rare_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fd0fb",
   "metadata": {},
   "source": [
    "### 2 - Fase di Pre-Processign per la Target Variable \"Destinazione d'uso\", applicando LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9170f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanzio in una nuova variabile il modello per l'encoding della target variable:\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Applico il modello di encoding alla mia target variable e salvo il risultato in una nuova colonna:\n",
    "df['Destinazione_Uso_Encoded'] = label_encoder.fit_transform(df['Destinazione_Uso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23faa5c3",
   "metadata": {},
   "source": [
    "### 3 - Filtro il data frame con solo le colonne che mi servono per la text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d04f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleziono le colonne da eliminare\n",
    "columns_to_drop = ['Destinazione_Uso', \n",
    "                   'Categoria_Catastale', \n",
    "                   'Tribunale', \n",
    "                   'Provincia', \n",
    "                   'Comune', \n",
    "                   'Numero_Lotto', \n",
    "                   'Numero_Vani', \n",
    "                   'Superficie', \n",
    "                   'Rapporto_Vani_Superficie', \n",
    "                   'N_Caratteri',\n",
    "                   'N_Parole', \n",
    "                   'avg_word_len',\n",
    "                   'Catasto_Fabbricati', \n",
    "                   'Descrizione_Bene',\n",
    "                   'Descrizione_Bene_Clean', \n",
    "                   'Descrizione_Bene_EDA']\n",
    "# Mantengo solo le colonne che mi servono per la text classification:\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b15d6",
   "metadata": {},
   "source": [
    "### 5 - Export final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "756f4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = df.to_csv(\"D:\\\\Master_Cefriel_DS_AI_Monolo\\\\0_Project_Work\\\\Dataset\\\\12_Dataset_XLM_RoBERTa_Model\\\\Dataset_XLM_RoBERTa_Model.csv\", \n",
    "                   index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
